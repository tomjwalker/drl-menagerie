# hello-drl-world

## DRL Algorithms
| Agent | Implemented? | Type | Pros | Cons |
| :---: | :---: | :---: | :---: | :---: |
| REINFORCE | wip | Policy Gradient | Smooth action probability distribution (vs e.g. discontinuous e-greedy); policy potentially simpler function to approximate than value functions; can approach deterministic policy | High variance (without baseline); sample inefficient; no guarantee of efficient exploration |
| SARSA |  |  |  |  |
| DQN |  |  |  |  |
| A2C |  |  |  |  |
| PPO |  |  |  |  |
| A3C |  |  |  |  |


## Nomenclature
| Symbol | Meaning |
| :---: | :---: |
| $\tau$ | **Trajectory**: a sequence of state-action-rewards, $s_0, a_0, r_0, ..., s_T, a_T, r_T$, sampled from a policy, $\tau \sim \pi$ |
| $\pi_{\theta}$ | **Policy (parametrised)**: A function which outputs stochastic actions, given a state: $a \sim \pi(s)$. Neural net used as function approximator, with learnable parameters $\theta$ |
| $R(\tau)$ | Return of a trajectory (at time step 0; if mid-way through an episode, subscripted by t; $R_t(\tau)$ |
| $J(\pi_{\theta})$ | Objective function: expected return over all trajectories generated by an agent |
| $\nabla_{\theta}J(\pi_{\theta})$ | Policy gradient: used in gradient ascent update equation to maximise the objective |
